{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70071661-da2d-4c4f-a092-f97733db89e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.56  Python-3.11.7 torch-2.5.1+cpu CPU (Intel Core(TM) Ultra 5 125H)\n",
      "Setup complete  (18 CPUs, 15.6 GB RAM, 162.7/399.9 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "from PIL import ImageEnhance\n",
    "import PIL.Image as Img\n",
    "import re\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "import os\n",
    "ultralytics.checks()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, default_data_collator, VisionEncoderDecoderModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd92513-076c-4054-98b4-50e4fe6d4ae0",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2057982-3e1b-4323-882a-bb0b26b22b3e",
   "metadata": {},
   "source": [
    "### Generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b49e637-54b6-4b22-af0b-4db329ed9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gt(gt_file_path):\n",
    "    with open(gt_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    coordinates = [d['Coordinate'] for d in data]\n",
    "    fields = [remove_special_char(d['Field name']) for d in data]\n",
    "    return fields, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde5735b-9ce5-497f-be3f-c9e5c3c1943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxA, boxB):\n",
    "    # Unpack coordinates\n",
    "    xA_min, yA_min, xA_max, yA_max = boxA\n",
    "    xB_min, yB_min, xB_max, yB_max = boxB\n",
    "\n",
    "    # Compute intersection coordinates\n",
    "    x_left = max(xA_min, xB_min)\n",
    "    y_top = max(yA_min, yB_min)\n",
    "    x_right = min(xA_max, xB_max)\n",
    "    y_bottom = min(yA_max, yB_max)\n",
    "\n",
    "    # Check for no overlap\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute intersection area\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # Compute areas of each box\n",
    "    boxA_area = (xA_max - xA_min) * (yA_max - yA_min)\n",
    "    boxB_area = (xB_max - xB_min) * (yB_max - yB_min)\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = intersection_area / float(boxA_area + boxB_area - intersection_area)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787315eb-b19c-4380-87f7-702ce85e63a9",
   "metadata": {},
   "source": [
    "### Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0261db8a-32d2-4784-8ddf-333a4c33f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo_inference(model, image_file):\n",
    "    results = model(image_file, conf=0.3, iou=0.4, save=True)\n",
    "    if not results[0].boxes is None:\n",
    "        boxes = results[0].boxes.xyxy.tolist()\n",
    "    else:\n",
    "        boxes = []\n",
    "        print(\"No handwritten text detected\")\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85655249-c51c-468a-823d-b31a9606168f",
   "metadata": {},
   "source": [
    "### Field Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd1c3332-1e87-4fe2-b7bb-4e49179a5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_coordinates(image_file):\n",
    "    image = Img.open(image_file)\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    contrast_image = enhancer.enhance(2)  \n",
    "    opencv_image = np.array(contrast_image) # Enhance Contrast \n",
    "    opencv_image = cv2.cvtColor(opencv_image, cv2.COLOR_RGB2BGR) # Convert RGB to BGR    \n",
    "    gray_image = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY) # Convert BGR to GRAY    \n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0) # Apply Blurr on Image    \n",
    "    edged = cv2.Canny(blurred_image, 150, 200) # Apply Canny Edge Detection\n",
    "    \n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    dilated_img = cv2.dilate(edged, kernel, iterations=1) # Perform dilation    \n",
    "    contours, hierarchy = cv2.findContours(dilated_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # Find contours\n",
    "\n",
    "    # Draw the contours on the original image\n",
    "    # cv2.drawContours(opencv_image, contours, -1, (0, 255, 0), 2) # Green color, thickness 2\n",
    "    # cv2.imwrite(\"Testimage.jpg\", opencv_image)\n",
    "    # cv2.imwrite(\"edged.jpg\", edged)\n",
    "    # cv2.imwrite(\"dilated.jpg\", dilated_img)\n",
    "#     plt.imshow(opencv_image)\n",
    "    # Calculate Contour Area\n",
    "    area = []\n",
    "    for cont in contours:\n",
    "        bbox_coord = cv2.boundingRect(cont)\n",
    "#         x1, x2, y1, y2 = x, x + w, y, y + h\n",
    "        area.append(bbox_coord[2] * bbox_coord[3])\n",
    "\n",
    "    \n",
    "    # Get Coordinates of Max Contour Area    \n",
    "    cnt = contours[np.argmax(area)]\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    x1, x2, y1, y2 = x, x + w, y, y + h\n",
    "    return [x1, y1, x2, y2] #, contours, hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affae692-0f23-4d38-b0df-c189dbff9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_coordinates(table_coord, field_coord):\n",
    "    cell_x1, cell_y1, cell_x2, cell_y2 = field_coord\n",
    "    table_x1, table_y1, table_x2, table_y2 = table_coord\n",
    "    \n",
    "    table_width = table_x2 - table_x1\n",
    "    table_height = table_y2 - table_y1\n",
    "    \n",
    "    norm_x1 = (cell_x1 - table_x1) / table_width\n",
    "    norm_y1 = (cell_y1 - table_y1) / table_height\n",
    "    norm_x2 = (cell_x2 - table_x1) / table_width\n",
    "    norm_y2 = (cell_y2 - table_y1) / table_height\n",
    "    \n",
    "    return [np.round(norm_x1, 4), np.round(norm_y1, 4), np.round(norm_x2, 4), np.round(norm_y2, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7524bd-5086-4a9d-bc79-af7e4064abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ref_coordinates(file_path):\n",
    "    ref_df = pd.read_excel(file_path)\n",
    "    ref_df.set_index('Field', inplace=True)\n",
    "    ref_table_coordinates = list(ref_df.loc['Full'])[:-2]\n",
    "    ref_field_coordinates = {}\n",
    "    ref_columns = list(ref_df.index)\n",
    "    for ind in range(1, len(ref_columns)):\n",
    "        f_coordinates = list(ref_df.loc[ref_columns[ind]])[:-2]        \n",
    "        ref_field_coordinates[ref_columns[ind]] = norm_coordinates(ref_table_coordinates, f_coordinates)\n",
    "    return ref_field_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1278377c-c006-4833-8a8b-754f0999821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(table_coordinates, field_coordinates):\n",
    "    pred_field = []\n",
    "    for cent in field_coordinates:\n",
    "        min_dist = {}\n",
    "        for key, val in table_coordinates.items():\n",
    "            min_dist[key] = compute_iou(cent, val)\n",
    "        pred_field.append(max(min_dist, key = lambda x: min_dist[x]))\n",
    "    \n",
    "    return pred_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38338227-bff7-4377-a727-4566fe63aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fields(image_file, ref_table_coordinates, pred_boxes):\n",
    "    local_table_coordinates = get_table_coordinates(image_file)    \n",
    "    field_coordinates = []\n",
    "    for coord in pred_boxes:\n",
    "        field_coordinates.append(norm_coordinates(local_table_coordinates, coord))\n",
    "    pred_fields = find_matches(ref_table_coordinates, field_coordinates)\n",
    "    pred_results = {f:c for f, c in zip(pred_fields, pred_boxes)}\n",
    "    return pred_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240bf15-230d-4975-9763-4844d3fc9850",
   "metadata": {},
   "source": [
    "### TrOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9660b65-6e6d-41ce-b8a7-861d1840cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ocr(image, processor, ocr_model):\n",
    "    # pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    # generated_ids = ocr_model.generate(pixel_values) # Generate output\n",
    "    # generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0] # Decode generated token ids to string\n",
    "    return \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "673ed111-f46f-456e-89e4-b046dbfdb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image_file, field_boxes, processor, ocr_model):\n",
    "    img = cv2.imread(image_file)\n",
    "    image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    field_text = {}\n",
    "    for field, box in field_boxes.items():\n",
    "        box = [int(coord) for coord in box]\n",
    "        start_x, start_y, end_x, end_y = box\n",
    "        cropped_image = image_rgb[start_y:end_y, start_x:end_x]\n",
    "        text = run_ocr(cropped_image, processor, ocr_model)\n",
    "        field_text[field] = post_processing([field, text])\n",
    "    return field_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b6120-cc91-4534-9f3d-6ca36adb28f0",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa5ac88c-846f-4ec9-828e-7aa8c493c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9]+', '', text)\n",
    "    return cleaned_text\n",
    "def numeric_only(text):\n",
    "    cleaned_text = re.sub(r'[^0-9]+', '', text)\n",
    "    return cleaned_text\n",
    "def char_space(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    return cleaned_text.strip()\n",
    "def generic(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9 +\\-]+', '', text)\n",
    "    return cleaned_text.strip()\n",
    "def alphabets_only(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z]+', '', text)\n",
    "    return cleaned_text.capitalize()\n",
    "def alphanum_specialchar(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9 +\\-/,.]+', '', text)\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e354d4e2-7bc8-407f-b642-d572b9d48ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_specific(text, camelcase=True):\n",
    "    text = re.sub(r'[^A-Za-z ,]+', '', text)\n",
    "    text = re.sub(r'^[^A-Za-z0-9]+|[^A-Za-z0-9]+$', '', text) # Remove special character at start and end\n",
    "    text = re.sub(r'[^A-Za-z0-9, ]+', ',', text) # Remove single special character to comma\n",
    "    text = re.sub(r'\\s+,', ',', text) # Remove extra spaces before comma\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if camelcase:\n",
    "            capital_letters = sum(1 for char in text if char.isupper())\n",
    "            if capital_letters <= len(word)//2:\n",
    "                # word = word.strip().lower()\n",
    "                word = word.capitalize().strip()\n",
    "            else:\n",
    "                word = word.strip()\n",
    "        else:\n",
    "            word = word.strip()\n",
    "        word = autocorrect_words(word.replace(\",\",\"\"), 'language') + \",\"\n",
    "        words.append(word)\n",
    "    return \" \".join(words)[:-1]\n",
    "    \n",
    "def date_format(text):\n",
    "    elements = text.split(\"/\")\n",
    "    corrected = []\n",
    "    if len(elements) == 3:\n",
    "        corrected = [str(int(ele)) for ele in elements]\n",
    "        return \"/\".join(corrected)\n",
    "    else:\n",
    "        return text\n",
    "        \n",
    "def date_specific(text):\n",
    "    cleaned_text = re.sub(r'[^0-9]+', '/', text)\n",
    "    cleaned_text = re.sub(r'/+', '/', cleaned_text)\n",
    "    cleaned_text = re.sub(r'^[^0-9]+|[^0-9]+$', '', cleaned_text)\n",
    "    cleaned_text = date_format(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "def bloodgroup_specific(text):\n",
    "    cleaned_text = re.sub(r'[^ABOTabot0-1+\\-]+', '', text)\n",
    "    cleaned_text = cleaned_text.replace('t', '+').replace('0', 'O')\n",
    "    return cleaned_text.upper().strip()\n",
    "\n",
    "def address_specific(text):\n",
    "    text = re.sub(r'^[^A-Za-z0-9]+|[^A-Za-z0-9]+$', '', text) # Remove special character at start and end\n",
    "    text = re.sub(r'[^A-Za-z0-9, /\\-.]+', ',', text) # Remove single special character to comma\n",
    "    text = re.sub(r'\\s+,', ',', text) # Remove extra spaces before comma\n",
    "    return text\n",
    "\n",
    "def reference_specific(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9 \\-]+', '', text)\n",
    "    words = cleaned_text.split(\"-\")\n",
    "    if len(words) == 1:        \n",
    "        cleaned_text = \" - \".join([re.sub(r'[^A-Za-z]+', '', text), re.sub(r'[^0-9]+', '', text)])\n",
    "    elif len(words) == 2:\n",
    "        cleaned_text = \" - \".join([process_words(words[0], True).strip(), re.sub(r'[^0-9]+', '', words[1])])    \n",
    "    return cleaned_text\n",
    "        \n",
    "def pan_specific(text):\n",
    "    text = remove_special_char(text)\n",
    "    text_transform = \"\"\n",
    "    if len(text) == 10:\n",
    "        text_transform = num2alpha(text[:5])\n",
    "        text_transform += alpha2num(text[5:-1])\n",
    "        text_transform += num2alpha(text[-1])\n",
    "    else:\n",
    "        text_transform = text    \n",
    "    return text_transform.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0a57b75-70b1-4949-b61b-f08694016a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/kaggle/input/supportingfiles/field_vocabulary.json', 'r') as f:\n",
    "    field_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "95ac0510-5da1-452d-a209-267a0ca2e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect_words(text, field):\n",
    "    accept_flag = False\n",
    "    if field in field_vocab:\n",
    "        training_labels = field_vocab[field]\n",
    "        dist = [levenshtein_distance(text, ele) for ele in training_labels]\n",
    "        \n",
    "        min_dist, min_dist_ind = np.min(dist), np.argmin(dist)\n",
    "        corrected = training_labels[min_dist_ind]\n",
    "        if ((min_dist <= 1) or (min_dist <= 2 and len(corrected) > 4) or (min_dist <= 3 and len(corrected) > 8)) and dist.count(min_dist) <= 1:\n",
    "            return corrected\n",
    "        else:\n",
    "            return text\n",
    "    else:\n",
    "        return text\n",
    "def process_words2(text):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.capitalize().strip()\n",
    "        word = autocorrect_words(word, field)\n",
    "        words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def process_words(text, camelcase=False):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        if camelcase:\n",
    "            capital_letters = sum(1 for char in text if char.isupper())\n",
    "            if capital_letters <= len(word)//2:\n",
    "                # word = word.strip().lower()\n",
    "                word = word.capitalize()\n",
    "            else:\n",
    "                word = word.strip()\n",
    "        else:\n",
    "            word = word.strip()\n",
    "        words.append(word)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f4bfa3c-bc8e-4b18-bb3f-720bfeca959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphatonum = {\"o\": \"0\", \"O\": \"0\", \"b\": \"6\", \"z\": \"2\", \"Z\": \"2\", \"B\": \"8\", \"G\": \"6\", \"I\": \"1\", \"T\":\"7\"}\n",
    "numtoalpha = {val: key for key, val in alphatonum.items()}\n",
    "schartonum = {\"/\": \"1\", \"\\\\\": \"1\", \"(\": \"1\"}\n",
    "\n",
    "\n",
    "def alpha2num(text):\n",
    "    text_transform = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            if char in alphatonum:\n",
    "                text_transform += alphatonum[char]\n",
    "            else:\n",
    "                text_transform += char\n",
    "        else:\n",
    "            text_transform += char\n",
    "    return text_transform\n",
    "\n",
    "def num2alpha(text):\n",
    "    text_transform = \"\"\n",
    "    for char in text:\n",
    "        if char.isdigit():\n",
    "            if char in numtoalpha:\n",
    "                text_transform += numtoalpha[char]\n",
    "            else:\n",
    "                text_transform += char\n",
    "        else:\n",
    "            text_transform += char\n",
    "    return text_transform\n",
    "\n",
    "def special2num(text):\n",
    "    text_transform = \"\"\n",
    "    for char in text:\n",
    "        if not char.isalnum():\n",
    "            if char in schartonum:\n",
    "                text_transform += schartonum[char]\n",
    "            else:\n",
    "                text_transform += char\n",
    "        else:\n",
    "            text_transform += char\n",
    "    return text_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ccfb23e-8ff7-4c8b-9af0-ac3364dfe414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(data):\n",
    "    if data[0] in ['Dateofbirth', 'date']:\n",
    "        return date_specific(data[1])\n",
    "    elif data[0] in ['nationality', 'gender', 'maritalstatus']:\n",
    "        clean_text = alphabets_only(data[1])\n",
    "        return autocorrect_words(clean_text, data[0])    \n",
    "    elif data[0] in ['candidatename', 'Fatherhusbandname']:\n",
    "        clean_text = char_space(data[1])\n",
    "        return process_words(clean_text, True)\n",
    "    elif data[0] in ['place']:\n",
    "        clean_text = alphabets_only(data[1])\n",
    "#         clean_text = process_words(clean_text, True)\n",
    "        return autocorrect_words(clean_text, data[0])\n",
    "    elif data[0] in ['contactnumber', 'AlternateNo', 'aadhaarcard']:\n",
    "        clean_text = special2num(alpha2num(data[1]))\n",
    "        return numeric_only(clean_text)\n",
    "    elif data[0] in ['permanentaddress', 'presentaddress']:\n",
    "        clean_text = alphanum_specialchar(data[1])\n",
    "        clean_text = address_specific(clean_text)\n",
    "        return process_words(clean_text, True)\n",
    "    elif data[0] in ['bloodgroup']:\n",
    "        return bloodgroup_specific(data[1])\n",
    "    elif data[0] in ['experience', 'experience1']:\n",
    "        clean_text = generic(data[1])\n",
    "        return process_words(clean_text, False)\n",
    "    elif data[0] in ['qualification']:\n",
    "        clean_text = generic(data[1])\n",
    "        clean_text = process_words(clean_text, True)        \n",
    "        return autocorrect_words(clean_text, data[0]) \n",
    "    elif data[0] in ['referencescmob1', 'referencescmob2']:\n",
    "        clean_text = reference_specific(data[1])\n",
    "        return clean_text\n",
    "    elif data[0] in ['pancard']:\n",
    "        return pan_specific(data[1])\n",
    "    elif data[0] in ['languageknown']:\n",
    "        return lang_specific(data[1])\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4205c-57e6-4735-9bb9-a931727637e4",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443fc99-becd-4d08-9026-72189fa1739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- METRICS & EFFICIENCY ---\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if isinstance(s1, str):\n",
    "        s1 = list(s1)\n",
    "    if isinstance(s2, str):\n",
    "        s2 = list(s2)\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def word_error_rate(y_true, y_pred):\n",
    "    total_words = 0\n",
    "    total_errors = 0\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        t_words = t.split()\n",
    "        p_words = p.split()\n",
    "        total_words += len(t_words)\n",
    "        total_errors += levenshtein_distance(t_words, p_words)\n",
    "    return total_errors / total_words if total_words > 0 else 0.0\n",
    "\n",
    "def char_error_rate(y_true, y_pred):\n",
    "    total_chars = 0\n",
    "    total_errors = 0\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        total_chars += len(t)\n",
    "        total_errors += levenshtein_distance(t, p)\n",
    "    return total_errors / total_chars if total_chars > 0 else 0.0\n",
    "\n",
    "def field_accuracy(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def document_level_accuracy(y_true, y_pred, doc_ids):\n",
    "    from collections import defaultdict\n",
    "    doc_true = defaultdict(list)\n",
    "    doc_pred = defaultdict(list)\n",
    "    for doc_id, t, p in zip(doc_ids, y_true, y_pred):\n",
    "        doc_true[doc_id].append(t)\n",
    "        doc_pred[doc_id].append(p)\n",
    "    correct_docs = 0\n",
    "    for doc_id in doc_true:\n",
    "        if doc_true[doc_id] == doc_pred[doc_id]:\n",
    "            correct_docs += 1\n",
    "    return correct_docs / len(doc_true) if doc_true else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07dde4-4a42-4cf5-8209-203192a0cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "831c52e2-2ef4-42cd-80dc-e078a37e7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_run = \"Run16\"\n",
    "# Path of model\n",
    "root_dir = r\"E:\\Nextstep\\MLChallenge\\DeHaDo_AI\"\n",
    "\n",
    "model_path = src_dir + \"/\" + r\"tokenwise-dehado-ai\\assets\\Models\\Custom_Coco\"\n",
    "\n",
    "model_name = \"best.pt\"\n",
    "# Path of train and test set\n",
    "train_path = src_dir + r\"\\Dataset\\train\"\n",
    "test_path = src_dir + r\"\\Dataset\\test\"\n",
    "\n",
    "# Results path\n",
    "res_path = src_dir + r\"\\Results\" + \"/\" + current_run\n",
    "## Predicted bounding box on Image\n",
    "output_path = res_path + \"\\Predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f464f00-8cbc-403a-b44a-40a0ed412c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e91f657b15f4d2d8eb90876dfddec83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muthukumar\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Muthukumar\\.cache\\huggingface\\hub\\models--microsoft--trocr-large-handwritten. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf36ff345b9d4a18b9f06e29c8f3943b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dff74868d4c4bf488916c0a72d787f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed01fca02834f1db54badf006283f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805947b4431245bf8167b95b285f6825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4624d8107959467b9c6e9691e7ad6db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6545df72154b798c1b996dc9d99ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ref_table_coordinates \u001b[38;5;241m=\u001b[39m load_ref_coordinates(root_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenwise-dehado-ai\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124massets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRef_Coordinates.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m processor \u001b[38;5;241m=\u001b[39m TrOCRProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/trocr-large-handwritten\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m VisionEncoderDecoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/trocr-large-handwritten\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:376\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    370\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFast initialization is currently not supported for VisionEncoderDecoderModel. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to slow initialization...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n\u001b[0;32m    374\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fast_init\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:3854\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3851\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3852\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   3853\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 3854\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3855\u001b[0m             pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs\n\u001b[0;32m   3856\u001b[0m         )\n\u001b[0;32m   3857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   3858\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   3859\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3860\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3861\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   3862\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   3863\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    404\u001b[0m         path_or_repo_id,\n\u001b[0;32m    405\u001b[0m         filename,\n\u001b[0;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    416\u001b[0m     )\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m    861\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    862\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    863\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m    864\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m    865\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    866\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    867\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    868\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m    869\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m    870\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m    871\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[0;32m    872\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    873\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m    875\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    876\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    877\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1009\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1007\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1009\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1010\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1011\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1012\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1013\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1014\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1015\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1016\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1017\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1020\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1541\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1543\u001b[0m     http_get(\n\u001b[0;32m   1544\u001b[0m         url_to_download,\n\u001b[0;32m   1545\u001b[0m         f,\n\u001b[0;32m   1546\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1547\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1548\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1549\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1550\u001b[0m     )\n\u001b[0;32m   1552\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1553\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:452\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    450\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    454\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = YOLO(model_path + \"/\" + model_name)\n",
    "ref_table_coordinates = load_ref_coordinates(root_dir + \"/\" + r\"tokenwise-dehado-ai\\assets\\Ref_Coordinates.xlsx\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\", use_fast=True)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d6d4dde-48ed-4ec2-8536-753767e2d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = test_path + \"/\" + \"images\"\n",
    "file = \"MIT_10.jpg\"\n",
    "file_path = image_path + \"/\" + file\n",
    "# pred_boxes = run_yolo_inference(model, file_path)\n",
    "pred_field_boxes = extract_fields(file_path, ref_table_coordinates, pred_boxes)\n",
    "pred_field_text = extract_text(file_path, pred_field_boxes, processor, ocr_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "432a4bce-b832-4833-84db-93c2d088ce07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'presentaddress': [907.2194213867188,\n",
       "  1282.40478515625,\n",
       "  1736.8446044921875,\n",
       "  1472.6346435546875],\n",
       " 'permanentaddress': [897.4985961914062,\n",
       "  1537.2169189453125,\n",
       "  2098.47119140625,\n",
       "  1768.4093017578125],\n",
       " 'experience1': [945.0428466796875,\n",
       "  1164.7982177734375,\n",
       "  1961.4866943359375,\n",
       "  1268.0960693359375],\n",
       " 'languageknown': [892.9874267578125,\n",
       "  1949.5166015625,\n",
       "  1684.2789306640625,\n",
       "  2053.94580078125],\n",
       " 'gender': [1824.88720703125,\n",
       "  850.718017578125,\n",
       "  2098.48291015625,\n",
       "  954.9454345703125],\n",
       " 'experience': [957.2333374023438,\n",
       "  1046.192626953125,\n",
       "  2355.276123046875,\n",
       "  1143.63720703125],\n",
       " 'referencescmob2': [966.2678833007812,\n",
       "  2185.861328125,\n",
       "  2140.096923828125,\n",
       "  2291.89892578125],\n",
       " 'nationality': [1845.9581298828125,\n",
       "  950.673583984375,\n",
       "  2151.87548828125,\n",
       "  1045.5997314453125],\n",
       " 'referencescmob1': [951.04541015625,\n",
       "  2066.5185546875,\n",
       "  1997.015869140625,\n",
       "  2173.115234375],\n",
       " 'Fatherhusbandname': [911.06982421875,\n",
       "  540.0111083984375,\n",
       "  1476.7900390625,\n",
       "  644.3064575195312],\n",
       " 'AlternateNo': [1877.0166015625,\n",
       "  1807.3072509765625,\n",
       "  2327.501220703125,\n",
       "  1908.927001953125],\n",
       " 'qualification': [906.37060546875,\n",
       "  751.8690185546875,\n",
       "  1346.808837890625,\n",
       "  854.3838500976562],\n",
       " 'Dateofbirth': [905.3399658203125,\n",
       "  643.9300537109375,\n",
       "  1376.214111328125,\n",
       "  744.6696166992188],\n",
       " 'candidatename': [912.2052612304688,\n",
       "  446.8509826660156,\n",
       "  1349.9140625,\n",
       "  545.938720703125],\n",
       " 'maritalstatus': [902.006103515625,\n",
       "  849.8128051757812,\n",
       "  1237.5726318359375,\n",
       "  955.2017822265625],\n",
       " 'date': [601.2288208007812,\n",
       "  2578.503662109375,\n",
       "  1110.3475341796875,\n",
       "  2677.413818359375],\n",
       " 'bloodgroup': [909.2534790039062,\n",
       "  956.8600463867188,\n",
       "  1017.4249877929688,\n",
       "  1030.8187255859375],\n",
       " 'place': [599.8563232421875,\n",
       "  2484.857177734375,\n",
       "  917.7354125976562,\n",
       "  2567.804931640625],\n",
       " 'contactnumber': [896.759033203125,\n",
       "  1823.921630859375,\n",
       "  1365.5467529296875,\n",
       "  1923.9854736328125],\n",
       " 'aadhaarcard': [595.1278686523438,\n",
       "  2392.6669921875,\n",
       "  1177.7181396484375,\n",
       "  2474.602783203125]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_field_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3d5c168-7cd2-442e-961a-61467919999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presentaddress 0.9469211891575613\n",
      "permanentaddress 0.9369870302371721\n",
      "experience1 0.8383726633315985\n",
      "languageknown 0.9534722547980123\n",
      "gender 0.9012471859446292\n",
      "experience 0.9211260247743872\n",
      "referencescmob2 0.8581930303199218\n",
      "nationality 0.9028368131749757\n",
      "referencescmob1 0.8609377627424383\n",
      "Fatherhusbandname 0.8347391507173885\n",
      "AlternateNo 0.8737551106463426\n",
      "qualification 0.8463382862225584\n",
      "Dateofbirth 0.8996616701850986\n",
      "candidatename 0.8804360776763397\n",
      "maritalstatus 0.8541920039297384\n",
      "date 0.9137287720109768\n",
      "bloodgroup 0.8275007835186984\n",
      "place 0.8855232400059172\n",
      "contactnumber 0.8521053171290589\n",
      "aadhaarcard 0.904218153451516\n"
     ]
    }
   ],
   "source": [
    "label_path = test_path + \"/\" + \"labels\"\n",
    "file = \"MIT_10.jpg\"\n",
    "fields, coords = load_gt(label_path + \"/\" + file.split(\".\")[0] + \".json\")\n",
    "act_box = {f:c for f, c in zip(fields,coords)}\n",
    "for key, val in pred_field_boxes.items():\n",
    "    # img = cv2.imread(file_path)\n",
    "    \n",
    "    # cv2.rectangle(img, (val[0], val[1]), (val[2], val[3]), (0, 255, 0), 2)\n",
    "    print(key, compute_iou(val, act_box[key]))\n",
    "    \n",
    "    # cv2.rectangle(img, (val[0], val[1]), (val[2], val[3]), (0, 255, 0), 2)\n",
    "# cv2.imwrite(\"Test.jpg\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37912bbc-6638-478d-a3f6-b17d41062cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
