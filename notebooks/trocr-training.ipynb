{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12079007,"sourceType":"datasetVersion","datasetId":7571578,"isSourceIdPinned":false}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets accelerate torchvision\n!pip install --upgrade accelerate\n!pip install --upgrade transformers\n!pip install -q sentencepiece\n!pip install -q jiwer\n!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T07:28:58.884574Z","iopub.execute_input":"2025-06-12T07:28:58.884793Z","iopub.status.idle":"2025-06-12T07:30:44.942441Z","shell.execute_reply.started":"2025-06-12T07:28:58.884771Z","shell.execute_reply":"2025-06-12T07:30:44.941515Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nCollecting accelerate\n  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.5.2\n    Uninstalling accelerate-1.5.2:\n      Successfully uninstalled accelerate-1.5.2\nSuccessfully installed accelerate-1.7.0\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\nSuccessfully installed transformers-4.52.4\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q transformers\n!pip install -q sentencepiece\n!pip install -q jiwer\n!pip install -q datasets\n!pip install -q evaluate\n!pip install -q -U accelerate\n\n!pip install -q matplotlib\n!pip install -q protobuf==3.20.1\n!pip install -q tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:16:58.616747Z","iopub.execute_input":"2025-06-12T08:16:58.616922Z","iopub.status.idle":"2025-06-12T08:18:54.298636Z","shell.execute_reply.started":"2025-06-12T08:16:58.616906Z","shell.execute_reply":"2025-06-12T08:18:54.297729Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\nonnx 1.17.0 requires protobuf>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-vision 3.10.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-videointelligence 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-iam 2.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-firestore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-functions 1.20.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-language 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-dataproc 5.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-spanner 3.53.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-aiplatform 1.87.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ntensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ngrpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\ngoogle-cloud-bigquery-connection 1.18.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-pubsub 2.29.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nfrom datasets import Dataset\nimport pandas as pd\nimport torch\nfrom transformers import TrOCRProcessor, default_data_collator\nimport evaluate\nimport numpy as np\nimport pandas as pd\nimport glob as glob\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n\n \n \nfrom PIL import Image\nfrom zipfile import ZipFile\nfrom tqdm.notebook import tqdm\nfrom dataclasses import dataclass\ndir_path = '/kaggle/input/groundtruthtext'\ntrain_path = dir_path + \"/\" + 'train/gt_text'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:30:44.495711Z","iopub.execute_input":"2025-06-12T08:30:44.496037Z","iopub.status.idle":"2025-06-12T08:30:44.501246Z","shell.execute_reply.started":"2025-06-12T08:30:44.496015Z","shell.execute_reply":"2025-06-12T08:30:44.500395Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(dir_path + \"/\" + \"train.csv\")\ntrain_df['image'] = train_df['image'].apply(lambda x: dir_path + \"/\" + x)\ntest_df = pd.read_csv(dir_path + \"/\" + \"test.csv\")\ntest_df['image'] = test_df['image'].apply(lambda x: dir_path + \"/\" + x)\n\ntrain_df.head()\nmerge_df = pd.concat([train_df.iloc[:4000], train_df.iloc[10000:12500], train_df.iloc[17500:19000]])\ndataset = Dataset.from_pandas(merge_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:32:20.441561Z","iopub.execute_input":"2025-06-12T08:32:20.441837Z","iopub.status.idle":"2025-06-12T08:32:20.506720Z","shell.execute_reply.started":"2025-06-12T08:32:20.441819Z","shell.execute_reply":"2025-06-12T08:32:20.506163Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch\n\ndef preprocess(example):\n    image = Image.open(example[\"image\"]).convert(\"RGB\")\n\n    # Process image\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values[0]#.half()\n    example[\"pixel_values\"] = pixel_values\n    # example[\"pixel_values\"] = pixel_values.tolist()  # Convert tensor to list\n\n    # Ensure text is not None\n    if example.get(\"text\") is None:\n        example[\"text\"] = \"\"\n\n    # Tokenize text\n    labels = processor.tokenizer(example[\"text\"], return_tensors=\"pt\", padding=\"max_length\", max_length=64).input_ids[0]\n    labels = [label if label != processor.tokenizer.pad_token_id else -100 for label in labels]\n    example[\"labels\"] = labels\n\n    return example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:32:23.809337Z","iopub.execute_input":"2025-06-12T08:32:23.810007Z","iopub.status.idle":"2025-06-12T08:32:23.815516Z","shell.execute_reply.started":"2025-06-12T08:32:23.809983Z","shell.execute_reply":"2025-06-12T08:32:23.814589Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# import shutil\n\n# def zip_directory(dir_path, output_path):\n#     shutil.make_archive(output_path, 'zip', dir_path)\n#     print(f\"Directory '{dir_path}' zipped successfully to '{output_path}.zip'\")\n\n# # Example usage\n# dir_to_zip = \"/kaggle/working/trocr-finetuned/output\"\n# output_zip_file = \"/kaggle/working/trocr-finetuned/out\"\n# zip_directory(dir_to_zip, output_zip_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T06:45:01.594267Z","iopub.execute_input":"2025-06-09T06:45:01.594917Z","iopub.status.idle":"2025-06-09T06:46:07.969311Z","shell.execute_reply.started":"2025-06-09T06:45:01.594892Z","shell.execute_reply":"2025-06-09T06:46:07.968536Z"}},"outputs":[{"name":"stdout","text":"Directory '/kaggle/working/trocr-finetuned/output' zipped successfully to '/kaggle/working/trocr-finetuned/out.zip'\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\", use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:31:56.532112Z","iopub.execute_input":"2025-06-12T08:31:56.532387Z","iopub.status.idle":"2025-06-12T08:31:57.355091Z","shell.execute_reply.started":"2025-06-12T08:31:56.532367Z","shell.execute_reply":"2025-06-12T08:31:57.354487Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"processed_dataset = dataset.map(preprocess)\n# processed_dataset.save_to_disk(f\"/kaggle/working/final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:32:30.981187Z","iopub.execute_input":"2025-06-12T08:32:30.981673Z","iopub.status.idle":"2025-06-12T08:36:32.163214Z","shell.execute_reply.started":"2025-06-12T08:32:30.981649Z","shell.execute_reply":"2025-06-12T08:36:32.162580Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6752907c72ae42289a0e898615af4a1b"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# from datasets import load_from_disk, concatenate_datasets\n# n_parts = 10\n# parts = [load_from_disk(f\"/kaggle/working/processed_part_{i}\") for i in range(n_parts)]\n# final_dataset = concatenate_datasets(parts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T10:57:06.627469Z","iopub.execute_input":"2025-06-06T10:57:06.628568Z","iopub.status.idle":"2025-06-06T10:57:07.023823Z","shell.execute_reply.started":"2025-06-06T10:57:06.628531Z","shell.execute_reply":"2025-06-06T10:57:07.022732Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"markdown","source":"# Method 1","metadata":{}},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass TrainingConfig:\n    BATCH_SIZE:    int = 48\n    EPOCHS:        int = 1\n    LEARNING_RATE: float = 0.00005\n \n@dataclass(frozen=True)\nclass DatasetConfig:\n    DATA_ROOT:     str = '/kaggle/input'\n \n@dataclass(frozen=True)\nclass ModelConfig:\n    MODEL_NAME: str = 'microsoft/trocr-base-handwritten'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:28.703135Z","iopub.execute_input":"2025-06-12T08:19:28.703435Z","iopub.status.idle":"2025-06-12T08:19:28.709732Z","shell.execute_reply.started":"2025-06-12T08:19:28.703413Z","shell.execute_reply":"2025-06-12T08:19:28.708943Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(DatasetConfig.DATA_ROOT, 'groundtruthtext/train.csv'))\ntest_df = pd.read_csv(os.path.join(DatasetConfig.DATA_ROOT, 'groundtruthtext/test.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:32.642721Z","iopub.execute_input":"2025-06-12T08:19:32.643064Z","iopub.status.idle":"2025-06-12T08:19:32.731071Z","shell.execute_reply.started":"2025-06-12T08:19:32.643038Z","shell.execute_reply":"2025-06-12T08:19:32.730436Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CustomOCRDataset(Dataset):\n    def __init__(self, root_dir, df, processor, max_target_length=64):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.max_target_length = max_target_length\n \n \n    def __len__(self):\n        return len(self.df)\n \n \n    def __getitem__(self, idx):\n        # The image file name.\n        file_name = self.df['image'][idx]\n        # The text (label).\n        text = self.df['text'][idx]\n        # Read the image, apply augmentations, and get the transformed pixels.\n        image = Image.open(self.root_dir + file_name).convert('RGB')\n        # image = train_transforms(image)\n        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n        # Pass the text through the tokenizer and get the labels,\n        # i.e. tokenized labels.\n        labels = self.processor.tokenizer(\n            text,\n            padding='max_length',\n            max_length=self.max_target_length\n        ).input_ids\n        # We are using -100 as the padding token.\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n        return encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:33.349889Z","iopub.execute_input":"2025-06-12T08:19:33.350205Z","iopub.status.idle":"2025-06-12T08:19:33.356418Z","shell.execute_reply.started":"2025-06-12T08:19:33.350182Z","shell.execute_reply":"2025-06-12T08:19:33.355558Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\ntrain_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'groundtruthtext/'),\n    df=train_df,\n    processor=processor\n)\nvalid_dataset = CustomOCRDataset(\n    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'groundtruthtext/'),\n    df=test_df,\n    processor=processor\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:36.742395Z","iopub.execute_input":"2025-06-12T08:19:36.742885Z","iopub.status.idle":"2025-06-12T08:19:39.057288Z","shell.execute_reply.started":"2025-06-12T08:19:36.742854Z","shell.execute_reply":"2025-06-12T08:19:39.056497Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"792f23b698284d28b4cdc03810ff9161"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9339e9a07ac74c2d99d87b9857c50c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a159320dd83475896a7c1b0714d615b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54d874239be4ee3b305ca8b7803130b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ed250824a24e6ab39b2a9432123027"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"encoding = train_dataset[0]\nfor k,v in encoding.items():\n    print(k, v.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:50.662557Z","iopub.execute_input":"2025-06-12T08:19:50.663300Z","iopub.status.idle":"2025-06-12T08:19:50.740276Z","shell.execute_reply.started":"2025-06-12T08:19:50.663273Z","shell.execute_reply":"2025-06-12T08:19:50.739630Z"}},"outputs":[{"name":"stdout","text":"pixel_values torch.Size([3, 384, 384])\nlabels torch.Size([64])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# # load dataset\n# from datasets import load_from_disk\n# from datasets import load_dataset\n# # processed_dataset = load_from_disk(\"/kaggle/input/processedgt/final/dataset_info.json\")\n# # len(processed_dataset)\n# processed_dataset = load_dataset('json', data_files='/kaggle/input/processedgt/final/dataset_info.json', streaming=True)\n# len(processed_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:51.610587Z","iopub.execute_input":"2025-06-12T08:19:51.611438Z","iopub.status.idle":"2025-06-12T08:19:51.614884Z","shell.execute_reply.started":"2025-06-12T08:19:51.611414Z","shell.execute_reply":"2025-06-12T08:19:51.614152Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, TrainerCallback\n\n# Load model\n# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\", use_fast=True)\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel.to(\"cuda\")\nprint(model)\n# Total parameters and trainable parameters.\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:19:56.748553Z","iopub.execute_input":"2025-06-12T08:19:56.748860Z","iopub.status.idle":"2025-06-12T08:20:08.593162Z","shell.execute_reply.started":"2025-06-12T08:19:56.748836Z","shell.execute_reply":"2025-06-12T08:20:08.592211Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6f91cddb030431fa9ce4b8d8ed054e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"513a11bb19b04fecb7ab52d13398c65c"}},"metadata":{}},{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742a0735764443fcb4a423bb65d37479"}},"metadata":{}},{"name":"stdout","text":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)\n333,921,792 total parameters.\n333,921,792 training parameters.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# from transformers import DataCollatorForSeq2Seq\n\n# data_collator = DataCollatorForSeq2Seq(\n#     processor.tokenizer, \n#     model=None,  # Set later after model is loaded\n#     padding=True, \n#     return_tensors=\"pt\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T06:46:55.485215Z","iopub.execute_input":"2025-06-12T06:46:55.485499Z","iopub.status.idle":"2025-06-12T06:46:55.489781Z","shell.execute_reply.started":"2025-06-12T06:46:55.485477Z","shell.execute_reply":"2025-06-12T06:46:55.488794Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# Set Correct vocab size.\nmodel.config.vocab_size = model.config.decoder.vocab_size\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\n \n \nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:11.103084Z","iopub.execute_input":"2025-06-12T08:20:11.103386Z","iopub.status.idle":"2025-06-12T08:20:11.108109Z","shell.execute_reply.started":"2025-06-12T08:20:11.103366Z","shell.execute_reply":"2025-06-12T08:20:11.107383Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(\n    model.parameters(), lr=0.00005, weight_decay=0.0005\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:14.369629Z","iopub.execute_input":"2025-06-12T08:20:14.369936Z","iopub.status.idle":"2025-06-12T08:20:16.085362Z","shell.execute_reply.started":"2025-06-12T08:20:14.369911Z","shell.execute_reply":"2025-06-12T08:20:16.084616Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# import evaluate\ncer_metric = evaluate.load('cer')\n \n \ndef compute_cer(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n \n \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n \n \n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n \n \n    return {\"cer\": cer}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:16.086406Z","iopub.execute_input":"2025-06-12T08:20:16.086647Z","iopub.status.idle":"2025-06-12T08:20:16.691431Z","shell.execute_reply.started":"2025-06-12T08:20:16.086622Z","shell.execute_reply":"2025-06-12T08:20:16.690832Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c69335988946ef9fa31c86bcf534bd"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    eval_strategy='epoch',\n    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n    fp16=True,\n    output_dir='seq2seq_model/',\n    logging_strategy='epoch',\n    save_strategy='epoch',\n    save_total_limit=5,\n    report_to='tensorboard',\n    num_train_epochs=TrainingConfig.EPOCHS\n)\n# training_args = Seq2SeqTrainingArguments(\n#     output_dir=\"./trocr-finetuned\",\n#     per_device_train_batch_size=48,\n#     # evaluation_strategy=\"epoch\",\n#     save_steps=500,\n#     logging_steps=50,\n#     num_train_epochs=3,\n#     fp16=True,\n#     predict_with_generate=True,\n#     report_to=\"none\",\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:18.653137Z","iopub.execute_input":"2025-06-12T08:20:18.653704Z","iopub.status.idle":"2025-06-12T08:20:18.682725Z","shell.execute_reply.started":"2025-06-12T08:20:18.653682Z","shell.execute_reply":"2025-06-12T08:20:18.682161Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class PrintLogsCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            print(logs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:21.055657Z","iopub.execute_input":"2025-06-12T08:20:21.055970Z","iopub.status.idle":"2025-06-12T08:20:21.060084Z","shell.execute_reply.started":"2025-06-12T08:20:21.055930Z","shell.execute_reply":"2025-06-12T08:20:21.059431Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import DefaultDataCollator\ndata_collator = DefaultDataCollator(return_tensors=\"pt\")\n# trainer = Seq2SeqTrainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=processed_dataset,\n#     compute_metrics=compute_cer,\n#     eval_dataset=processed_dataset.select(range(20)),  # Optional\n#     data_collator=default_data_collator,\n#     tokenizer=processor.tokenizer,\n# )\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.tokenizer,\n    args=training_args,\n    compute_metrics=compute_cer,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:22.082600Z","iopub.execute_input":"2025-06-12T08:20:22.082894Z","iopub.status.idle":"2025-06-12T08:20:22.399726Z","shell.execute_reply.started":"2025-06-12T08:20:22.082873Z","shell.execute_reply":"2025-06-12T08:20:22.399167Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3932816936.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n\n# trainer.add_callback(PrintLogsCallback())\nres = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:20:26.373260Z","iopub.execute_input":"2025-06-12T08:20:26.373546Z","iopub.status.idle":"2025-06-12T08:20:26.910313Z","shell.execute_reply.started":"2025-06-12T08:20:26.373527Z","shell.execute_reply":"2025-06-12T08:20:26.909299Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1017692171.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# trainer.add_callback(PrintLogsCallback())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_xla_v2_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_spmd_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_unused_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_collator_with_removed_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0msignature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signature_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0mignored_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignored_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mdset_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\"in the {description} set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mcolumn_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m         \"\"\"\n\u001b[0;32m-> 1824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'CustomOCRDataset' object has no attribute '_data'"],"ename":"AttributeError","evalue":"'CustomOCRDataset' object has no attribute '_data'","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"# Method 2","metadata":{}},{"cell_type":"code","source":"processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\", use_fast=True)\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\nmodel.to(\"cuda\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:37:40.256510Z","iopub.execute_input":"2025-06-12T08:37:40.257227Z","iopub.status.idle":"2025-06-12T08:37:42.186643Z","shell.execute_reply.started":"2025-06-12T08:37:40.257202Z","shell.execute_reply":"2025-06-12T08:37:42.185990Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"VisionEncoderDecoderModel(\n  (encoder): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): TrOCRForCausalLM(\n    (model): TrOCRDecoderWrapper(\n      (decoder): TrOCRDecoder(\n        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (layers): ModuleList(\n          (0-11): 12 x TrOCRDecoderLayer(\n            (self_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): TrOCRAttention(\n              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n  )\n)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(\n    processor.tokenizer, \n    model=model,  # Set later after model is loaded\n    padding=True, \n    return_tensors=\"pt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:37:46.336498Z","iopub.execute_input":"2025-06-12T08:37:46.337095Z","iopub.status.idle":"2025-06-12T08:37:46.340573Z","shell.execute_reply.started":"2025-06-12T08:37:46.337070Z","shell.execute_reply":"2025-06-12T08:37:46.339849Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# Set Correct vocab size.\nmodel.config.vocab_size = model.config.decoder.vocab_size\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\n \n \nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:37:50.057585Z","iopub.execute_input":"2025-06-12T08:37:50.058151Z","iopub.status.idle":"2025-06-12T08:37:50.062631Z","shell.execute_reply.started":"2025-06-12T08:37:50.058127Z","shell.execute_reply":"2025-06-12T08:37:50.061903Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"cer_metric = evaluate.load('cer')\n \n \ndef compute_cer(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n \n \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n \n \n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n \n \n    return {\"cer\": cer}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T08:37:53.231073Z","iopub.execute_input":"2025-06-12T08:37:53.231765Z","iopub.status.idle":"2025-06-12T08:37:53.685632Z","shell.execute_reply.started":"2025-06-12T08:37:53.231741Z","shell.execute_reply":"2025-06-12T08:37:53.685102Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./trocr-finetuned\",\n    per_device_train_batch_size=2,\n    eval_strategy=\"steps\",\n    save_steps=100,\n    logging_steps=20,\n    eval_steps = 20,\n    num_train_epochs=3,\n    fp16=True,\n    predict_with_generate=True,\n    report_to=\"none\",\n    gradient_accumulation_steps=24\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:25:44.993168Z","iopub.execute_input":"2025-06-12T09:25:44.993771Z","iopub.status.idle":"2025-06-12T09:25:45.023356Z","shell.execute_reply.started":"2025-06-12T09:25:44.993747Z","shell.execute_reply":"2025-06-12T09:25:45.022500Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_dataset,\n    compute_metrics=compute_cer,\n    eval_dataset=processed_dataset.select(range(50)),  # Optional\n    data_collator=default_data_collator,\n    tokenizer=processor.tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:25:46.350511Z","iopub.execute_input":"2025-06-12T09:25:46.351001Z","iopub.status.idle":"2025-06-12T09:25:46.370348Z","shell.execute_reply.started":"2025-06-12T09:25:46.350978Z","shell.execute_reply":"2025-06-12T09:25:46.369753Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/798788004.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(\n    model.parameters(), lr=0.00005, weight_decay=0.0005\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:25:55.937008Z","iopub.execute_input":"2025-06-12T09:25:55.937551Z","iopub.status.idle":"2025-06-12T09:25:55.944470Z","shell.execute_reply.started":"2025-06-12T09:25:55.937527Z","shell.execute_reply":"2025-06-12T09:25:55.943679Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"res = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:25:58.742015Z","iopub.execute_input":"2025-06-12T09:25:58.742477Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='461' max='498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [461/498 2:33:29 < 12:22, 0.05 it/s, Epoch 2.77/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Cer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.860400</td>\n      <td>2.423681</td>\n      <td>0.910690</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.566500</td>\n      <td>2.128994</td>\n      <td>1.200271</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.499800</td>\n      <td>1.792621</td>\n      <td>1.453315</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.257000</td>\n      <td>1.689292</td>\n      <td>0.928281</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.972000</td>\n      <td>1.594626</td>\n      <td>1.110961</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.895100</td>\n      <td>1.315072</td>\n      <td>0.906631</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.862000</td>\n      <td>1.108550</td>\n      <td>0.745602</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.774000</td>\n      <td>1.148231</td>\n      <td>0.920162</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.541700</td>\n      <td>1.056035</td>\n      <td>0.768606</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.312600</td>\n      <td>0.953724</td>\n      <td>0.947226</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.275200</td>\n      <td>0.922446</td>\n      <td>0.591340</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.231200</td>\n      <td>0.798173</td>\n      <td>0.592693</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.187000</td>\n      <td>0.823168</td>\n      <td>0.591340</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.238100</td>\n      <td>0.728094</td>\n      <td>0.594046</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.145400</td>\n      <td>0.671455</td>\n      <td>0.650880</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.109100</td>\n      <td>0.631621</td>\n      <td>0.569689</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.978300</td>\n      <td>0.554724</td>\n      <td>0.548038</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.807300</td>\n      <td>0.533767</td>\n      <td>0.545332</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.848600</td>\n      <td>0.485646</td>\n      <td>0.572395</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.799000</td>\n      <td>0.476058</td>\n      <td>0.572395</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.780600</td>\n      <td>0.449790</td>\n      <td>0.546685</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.772900</td>\n      <td>0.425297</td>\n      <td>0.476319</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.784800</td>\n      <td>0.430302</td>\n      <td>0.469553</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./trocr-finetuned/output\")\nprocessor.save_pretrained(\"./trocr-finetuned/output\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T06:43:41.837627Z","iopub.execute_input":"2025-06-09T06:43:41.838385Z","iopub.status.idle":"2025-06-09T06:43:44.474547Z","shell.execute_reply.started":"2025-06-09T06:43:41.838361Z","shell.execute_reply":"2025-06-09T06:43:44.473614Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# processor = TrOCRProcessor.from_pretrained(\"./trocr-finetuned\")\n# model = VisionEncoderDecoderModel.from_pretrained(\"./trocr-finetuned\")\n\n# # Move model to GPU if available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# Load and preprocess the image\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"# folder_path = train_path + \"/\" + \"MIT_1\"\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# for file in os.listdir(folder_path):        \n    \n#     image_path = folder_path + \"/\" + file\n#     image = Image.open(image_path).convert(\"RGB\")\n#     pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device).half()\n    \n#     # Generate output\n#     generated_ids = model.generate(pixel_values)\n    \n#     # Decode generated token ids to string\n#     generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n#     print(\"Predicted Text:\", generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T06:54:04.429138Z","iopub.execute_input":"2025-06-09T06:54:04.429839Z","iopub.status.idle":"2025-06-09T06:54:10.764666Z","shell.execute_reply.started":"2025-06-09T06:54:04.429816Z","shell.execute_reply":"2025-06-09T06:54:10.764064Z"}},"outputs":[{"name":"stdout","text":"Predicted Text: Indian-\nPredicted Text: 12271975\nPredicted Text: Lhhak Bak\nPredicted Text: HNo 23 23 Ch Marg,,alla,ad,ada,ad-6449\nPredicted Text: 9 atan Lan Lad\nPredicted Text: 06171720\nPredicted Text: H...No 13 B Circle Circle Circle, Circle,, Circle Circle English,,hing,,ig,,ugu\nPredicted Text: Female\nPredicted Text: A\nPredicted Text: 10 athab Chal P\nPredicted Text: PostGrad\nPredicted Text: Mar English English Beng,ugu,ugu Hindi Hindi Hindi\nPredicted Text: 35000000000000000..0..\nPredicted Text: I, -3638 -38382838\nPredicted Text: Mar\nPredicted Text: 9310549969\nPredicted Text: 97806161628\nPredicted Text: Dehman\nPredicted Text: Daylashishi\nPredicted Text: Kik B -99999940644\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# processor1 = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\", use_fast=True)\n# model1 = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n# model1.to(\"cuda\")\n# folder_path = train_path + \"/\" + \"MIT_1\"\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# for file in os.listdir(folder_path):        \n    \n#     image_path = folder_path + \"/\" + file\n#     image = Image.open(image_path).convert(\"RGB\")\n#     pixel_values = processor1(images=image, return_tensors=\"pt\").pixel_values.to(device).half()\n    \n#     # Generate output\n#     generated_ids = model1.generate(pixel_values)\n    \n#     # Decode generated token ids to string\n#     generated_text = processor1.batch_decode(generated_ids, skip_special_tokens=True)[0]\n#     print(\"Predicted Text:\", generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T06:56:30.406395Z","iopub.execute_input":"2025-06-09T06:56:30.406880Z","iopub.status.idle":"2025-06-09T06:56:41.786352Z","shell.execute_reply.started":"2025-06-09T06:56:30.406858Z","shell.execute_reply":"2025-06-09T06:56:41.785715Z"}},"outputs":[{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"pooler_act\": \"tanh\",\n  \"pooler_output_size\": 768,\n  \"qkv_bias\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Predicted Text: Indian\nPredicted Text: 12I 27(1975\nPredicted Text: Laksh Bakshi\nPredicted Text: H.NO. 23 , Chaudry Path , Nadjad - 5594 07\nPredicted Text: a year's at Denian Lod\nPredicted Text: ob ( 17 ( 2023\nPredicted Text: # 13 3 , Bala circle , Bogggigan\nPredicted Text: Female\nPredicted Text: a t\nPredicted Text: 10 years , at Chaka ) PLC\nPredicted Text: Post- Graduate\nPredicted Text: Marathi , English , Delugu , Hindi\nPredicted Text: 35808600000\nPredicted Text: Janaki Handa - 8634823848\nPredicted Text: married\nPredicted Text: 9351 045691\nPredicted Text: 978 7612803 .\nPredicted Text: Dehradun .\nPredicted Text: Dayita Bakshi\nPredicted Text: Kritika Brar - 7099406444\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# processor1 = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\", use_fast=True)\n# model1 = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\n# model1.to(\"cuda\")\nfolder_path = train_path + \"/\" + \"MIT_1\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor file in os.listdir(folder_path):        \n    \n    image_path = folder_path + \"/\" + file\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device).half()\n    \n    # Generate output\n    generated_ids = model.generate(pixel_values)\n    \n    # Decode generated token ids to string\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    print(\"Predicted Text:\", generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:11:39.940907Z","iopub.execute_input":"2025-06-12T09:11:39.941640Z","iopub.status.idle":"2025-06-12T09:11:47.440943Z","shell.execute_reply.started":"2025-06-12T09:11:39.941609Z","shell.execute_reply":"2025-06-12T09:11:47.440345Z"}},"outputs":[{"name":"stdout","text":"Predicted Text: Indian\nPredicted Text: 1212112121212212\nPredicted Text: Lhakhi\nPredicted Text: H... Che,,, Chow,, -\nPredicted Text: 99 999949996 Ltd Ltd Ltd\nPredicted Text: 06060660606230306062020230620066230666606630606260623620060616066203060306036616060603\nPredicted Text: H33.. B B B,, B\nPredicted Text: Female\nPredicted Text: AAA\nPredicted Text: 101010110101101010 1010101110104 P1010301010161010101LC\nPredicted Text: Post/ G\nPredicted Text: Mar,ati,,, English,,ali, Hindi,, Hindi Hindi, Hindi\nPredicted Text: 3000000\nPredicted Text: I H H -3838\nPredicted Text: Mar\nPredicted Text: 9300561\nPredicted Text: 9181820\nPredicted Text: DeDeDeunun\nPredicted Text: DDitahii\nPredicted Text: Kalitaarar -4949\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"cer_metric.compute(predictions='1212112121', references='12/27/1975')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:13:49.740385Z","iopub.execute_input":"2025-06-12T09:13:49.740939Z","iopub.status.idle":"2025-06-12T09:13:49.751528Z","shell.execute_reply.started":"2025-06-12T09:13:49.740918Z","shell.execute_reply":"2025-06-12T09:13:49.750910Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"0.7"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"len('06060660606230306062020230620066230666606630606260623620060616066203060306036616060603')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:25:24.403063Z","iopub.execute_input":"2025-06-12T09:25:24.403332Z","iopub.status.idle":"2025-06-12T09:25:24.408380Z","shell.execute_reply.started":"2025-06-12T09:25:24.403311Z","shell.execute_reply":"2025-06-12T09:25:24.407720Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"86"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}