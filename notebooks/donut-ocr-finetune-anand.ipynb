{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24aabea",
   "metadata": {},
   "source": [
    "<H3># Donut Model<H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f486abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anand\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anand\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
      "Downloading huggingface_hub-0.32.0-py3-none-any.whl (509 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/7 [tqdm]\n",
      "   ----------- ---------------------------- 2/7 [regex]\n",
      "   ----------------- ---------------------- 3/7 [pyyaml]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------- ----------------- 4/7 [huggingface-hub]\n",
      "   ---------------------------- ----------- 5/7 [tokenizers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------- ----- 6/7 [transformers]\n",
      "   ---------------------------------------- 7/7 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.32.0 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 #cuda 11.8\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 #cuda 12.6\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 #cuda 12.1\n",
    "!pip3 install ipykernel ipython IProgress jupyter ultralytics json numpy transformers torchvision accelerate datasets sentencepiece pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA devices: 2\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    print(\"CUDA devices:\", torch.cuda.device_count())        \n",
    "    torch_device = torch.device(\"cuda:0\")\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f5fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: datasets in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (0.32.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (2.7.0+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (0.32.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from torch>=2.0.0->accelerate) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\anand\\miniconda3\\envs\\torch_env2\\lib\\site-packages (6.31.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade datasets\n",
    "!pip install --upgrade accelerate\n",
    "#!pip install accelerate==0.30.0\n",
    "!pip install tf-keras\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy<2.0.0\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import tqdm as notebook_tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67048d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipykernel\n",
    "import json\n",
    "\n",
    "RESULT_DIR = 'results7'\n",
    "IMAGES_DIR = r'D:\\AI_Challenge\\DeHaDo-AI\\DEHADO-AI_TRAINING_DATASET_PHASE_I\\IMAGES_750'\n",
    "GT_LABELS_DIR = r'D:\\AI_Challenge\\DeHaDo-AI\\DEHADO-AI_TRAINING_DATASET_PHASE_I\\LABELS_750'\n",
    "\n",
    "def read_gt_text(txt_path):\n",
    "    \"\"\"\n",
    "    Read ground truth JSON file and return list of text values.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    if not os.path.exists(txt_path):\n",
    "        return result # Return empty list if file doesn't exist\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # Assuming `data` is a list where each element is an object containing 'Field value'            \n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if isinstance(item, dict) and 'Field value' in item:\n",
    "                        value = item['Field value']\n",
    "                        if isinstance(value, str):\n",
    "                            result.append(value.strip())\n",
    "                        elif isinstance(value, list) and value: # Handle lists of strings\n",
    "                            result.append(' '.join(map(str, value)).strip()) # Join elements if it's a list\n",
    "                        else:\n",
    "                            result.append('') # Append empty string for unexpected formats\n",
    "                    else:\n",
    "                        print(f\"Warning: Skipping unexpected item format in JSON file: {item}\")                        \n",
    "                        pass\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from {txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading ground truth text from {txt_path}: {e}\")\n",
    "\n",
    "    return result # Return a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Fine-tune Donut Transformer for Document OCR\n",
    "\n",
    "\n",
    "# --- Prepare data for Donut ---\n",
    "# Each sample: {\"image\": <PIL.Image>, \"text\": <target string>}\n",
    "\n",
    "def build_donut_dataset(images_dir, ocr_txt_dir):\n",
    "    samples = []\n",
    "    img_list = [] \n",
    "    img_list = os.listdir(images_dir)\n",
    "\n",
    "    for img_file in img_list[:50]:\n",
    "        if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        base_filename = os.path.splitext(img_file)[0]\n",
    "        gt_txt_path = os.path.join(ocr_txt_dir, base_filename + '.json')\n",
    "        gt_texts = read_gt_text(gt_txt_path)\n",
    "        # For Donut, we want the full image and all fields as a single string\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            target_text = \"\\n\".join(gt_texts)\n",
    "            samples.append({\"image\": img, \"text\": target_text})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "    return samples\n",
    "\n",
    "donut_samples = build_donut_dataset(IMAGES_DIR, GT_LABELS_DIR)\n",
    "donut_dataset = Dataset.from_list(donut_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a89cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderConfig {\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"batch_size\": 8,\n",
      "  \"decoder\": {\n",
      "    \"activation_dropout\": 0.0,\n",
      "    \"activation_function\": \"gelu\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"add_final_layer_norm\": true,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"classifier_dropout\": 0.0,\n",
      "    \"d_model\": 1024,\n",
      "    \"decoder_attention_heads\": 16,\n",
      "    \"decoder_ffn_dim\": 4096,\n",
      "    \"decoder_layerdrop\": 0.0,\n",
      "    \"decoder_layers\": 4,\n",
      "    \"dropout\": 0.1,\n",
      "    \"encoder_attention_heads\": 16,\n",
      "    \"encoder_ffn_dim\": 4096,\n",
      "    \"encoder_layerdrop\": 0.0,\n",
      "    \"encoder_layers\": 12,\n",
      "    \"init_std\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"max_position_embeddings\": 1536,\n",
      "    \"model_type\": \"mbart\",\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"scale_embedding\": true,\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 57525\n",
      "  },\n",
      "  \"encoder\": {\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"depths\": [\n",
      "      2,\n",
      "      2,\n",
      "      14,\n",
      "      2\n",
      "    ],\n",
      "    \"drop_path_rate\": 0.1,\n",
      "    \"embed_dim\": 128,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": [\n",
      "      2560,\n",
      "      1920\n",
      "    ],\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"mlp_ratio\": 4.0,\n",
      "    \"model_type\": \"donut-swin\",\n",
      "    \"num_channels\": 3,\n",
      "    \"num_heads\": [\n",
      "      4,\n",
      "      8,\n",
      "      16,\n",
      "      32\n",
      "    ],\n",
      "    \"num_layers\": 4,\n",
      "    \"patch_size\": 4,\n",
      "    \"path_norm\": true,\n",
      "    \"qkv_bias\": true,\n",
      "    \"torch_dtype\": \"float32\",\n",
      "    \"use_absolute_embeddings\": false,\n",
      "    \"window_size\": 10\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"learning_rate\": 5e-05,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"num_train_epochs\": 10,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained configuration\n",
    "config = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\").config\n",
    "\n",
    "# Adjust hyperparameters\n",
    "config.learning_rate = 5e-5  # Reduce learning rate for fine-tuning\n",
    "config.batch_size = 8       # Set batch size based on GPU capacity\n",
    "config.num_train_epochs = 10 # Number of epochs\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Map: 100%|██████████| 50/50 [01:46<00:00,  2.13s/ examples]\n"
     ]
    }
   ],
   "source": [
    "# --- Load Donut Processor and Model ---\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\",config=config)\n",
    "\n",
    "# Fix: Set decoder_start_token_id if not set\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "# Fix: Set pad_token_id if not set\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "model.to(torch_device)\n",
    "\n",
    "# --- Preprocess function ---\n",
    "def preprocess_donut(example):\n",
    "    pixel_values = processor(example[\"image\"], return_tensors=\"pt\").pixel_values[0]\n",
    "    labels = processor.tokenizer(\n",
    "        example[\"text\"], add_special_tokens=True, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Ignore padding in loss\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "donut_dataset = donut_dataset.map(preprocess_donut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee21c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split train/test ---\n",
    "donut_dataset = donut_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = donut_dataset[\"train\"]\n",
    "eval_dataset = donut_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Arguments ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./donut-ocr-finetune\",\n",
    "    per_device_train_batch_size=1,  # Reduce from 2 to 1\n",
    "    per_device_eval_batch_size=1,   # Reduce from 2 to 1\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33fc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:38: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA OOM error caught. Emptying cache and retrying...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py\", line 525, in forward\n    encoder_outputs = self.encoder(\n        pixel_values=pixel_values,\n    ...<3 lines>...\n        **kwargs_encoder,\n    )\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 968, in forward\n    encoder_outputs = self.encoder(\n        embedding_output,\n    ...<4 lines>...\n        return_dict=return_dict,\n    )\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 829, in forward\n    layer_outputs = layer_module(\n        hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n    )\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 748, in forward\n    layer_outputs = layer_module(\n        hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n    )\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 677, in forward\n    attention_outputs = self.attention(\n        hidden_states_windows, attn_mask, head_mask, output_attentions=output_attentions\n    )\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 547, in forward\n    self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 458, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Of the allocated memory 22.51 GiB is allocated by PyTorch, and 124.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 55\u001b[39m\n",
      "\u001b[32m     54\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     56\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n",
      "\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n",
      "\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n",
      "\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n",
      "\u001b[32m   2561\u001b[39m ):\n",
      "\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mDetachLossSeq2SeqTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, hiddens)\u001b[39m\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs,hiddens):\n",
      "\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Standard training step\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     loss = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Clear CUDA cache after each batch to help avoid OOM\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mDetachLossSeq2SeqTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n",
      "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Detach loss from computation graph to avoid memory leaks\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n",
      "\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n",
      "\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n",
      "\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n",
      "\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n",
      "\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:127\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n",
      "\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    128\u001b[39m outputs.append(output)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: Caught OutOfMemoryError in replica 1 on device 1.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 97, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py\", line 525, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "        pixel_values=pixel_values,\n",
      "    ...<3 lines>...\n",
      "        **kwargs_encoder,\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 968, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "        embedding_output,\n",
      "    ...<4 lines>...\n",
      "        return_dict=return_dict,\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 829, in forward\n",
      "    layer_outputs = layer_module(\n",
      "        hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 748, in forward\n",
      "    layer_outputs = layer_module(\n",
      "        hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 701, in forward\n",
      "    layer_output = self.intermediate(layer_output)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 565, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\activations.py\", line 69, in forward\n",
      "    return self.act(input)\n",
      "           ~~~~~~~~^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 1 has a total capacity of 11.00 GiB of which 0 bytes is free. Of the allocated memory 9.86 GiB is allocated by PyTorch, and 220.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 61\u001b[39m\n",
      "\u001b[32m     59\u001b[39m     torch.cuda.empty_cache()\n",
      "\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# Optionally, you can retry or exit gracefully here\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Uncomment to retry (may OOM again if batch size is too large)\u001b[39;00m\n",
      "\u001b[32m     62\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n",
      "\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n",
      "\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[32m   2548\u001b[39m context = (\n",
      "\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n",
      "\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n",
      "\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n",
      "\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n",
      "\u001b[32m   2553\u001b[39m )\n",
      "\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n",
      "\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n",
      "\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n",
      "\u001b[32m   2561\u001b[39m ):\n",
      "\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mDetachLossSeq2SeqTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, hiddens)\u001b[39m\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs,hiddens):\n",
      "\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Standard training step\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     loss = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Clear CUDA cache after each batch to help avoid OOM\u001b[39;00m\n",
      "\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n",
      "\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n",
      "\u001b[32m   3751\u001b[39m ):\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mDetachLossSeq2SeqTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n",
      "\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n",
      "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Detach loss from computation graph to avoid memory leaks\u001b[39;00m\n",
      "\u001b[32m      7\u001b[39m     loss = outputs.loss.detach()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n",
      "\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n",
      "\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n",
      "\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n",
      "\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n",
      "\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n",
      "\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:127\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n",
      "\u001b[32m    125\u001b[39m     output = results[i]\n",
      "\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    128\u001b[39m     outputs.append(output)\n",
      "\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n",
      "\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n",
      "\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: Caught OutOfMemoryError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 97, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py\", line 525, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "        pixel_values=pixel_values,\n",
      "    ...<3 lines>...\n",
      "        **kwargs_encoder,\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 968, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "        embedding_output,\n",
      "    ...<4 lines>...\n",
      "        return_dict=return_dict,\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 829, in forward\n",
      "    layer_outputs = layer_module(\n",
      "        hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 748, in forward\n",
      "    layer_outputs = layer_module(\n",
      "        hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 677, in forward\n",
      "    attention_outputs = self.attention(\n",
      "        hidden_states_windows, attn_mask, head_mask, output_attentions=output_attentions\n",
      "    )\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 547, in forward\n",
      "    self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\Anand\\miniconda3\\envs\\torch_env2\\Lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py\", line 458, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 23.99 GiB of which 0 bytes is free. Of the allocated memory 22.51 GiB is allocated by PyTorch, and 124.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class DetachLossSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        # Detach loss from computation graph to avoid memory leaks\n",
    "        loss = outputs.loss.detach()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs,hiddens):\n",
    "        # Standard training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "        # Clear CUDA cache after each batch to help avoid OOM\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def evaluation_loop(\n",
    "        self,\n",
    "        dataloader,\n",
    "        description,\n",
    "        prediction_loss_only=None,\n",
    "        ignore_keys=None,\n",
    "        metric_key_prefix: str = \"eval\",\n",
    "    ):\n",
    "        # Optionally clear cache after each evaluation batch as well\n",
    "        output = super().evaluation_loop(\n",
    "            dataloader,\n",
    "            description,\n",
    "            prediction_loss_only=prediction_loss_only,\n",
    "            ignore_keys=ignore_keys,\n",
    "            metric_key_prefix=metric_key_prefix,\n",
    "        )\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "        return output\n",
    "\n",
    "# --- Trainer ---\n",
    "def data_collator(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(x[\"pixel_values\"]) if not isinstance(x[\"pixel_values\"], torch.Tensor) else x[\"pixel_values\"] for x in batch])\n",
    "    labels = torch.stack([torch.tensor(x[\"labels\"]) if not isinstance(x[\"labels\"], torch.Tensor) else x[\"labels\"] for x in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "trainer = DetachLossSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=processor.tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# --- Fine-tune ---\n",
    "try:\n",
    "    trainer.train()\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"CUDA OOM error caught. Emptying cache and retrying...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # Optionally, you can retry or exit gracefully here\n",
    "        trainer.train()  # Uncomment to retry (may OOM again if batch size is too large)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Optionally, clear cache after training as well\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./fine_tuned_donut\"\n",
    "model.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# --- Inference Example ---\n",
    "def donut_ocr_infer(image_path, processor, model):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(img, return_tensors=\"pt\").pixel_values\n",
    "    outputs = model.generate(pixel_values.to(model.device), max_length=512)\n",
    "    result = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result\n",
    "\n",
    "TEST_IMAGE = r'D:\\AI_Challenge\\DeHaDo-AI\\DEHADO-AI_TRAINING_DATASET\\IMAGES_750\\MIT_1.jpg'\n",
    "# Example usage:\n",
    "result_text = donut_ocr_infer(TEST_IMAGE, processor, model)\n",
    "print(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7960a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Export to ONNX\n",
    "dummy_input = torch.rand(1, 3, 256, 256)  # Replace with your input size\n",
    "onnx_path = \"./donut_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    ")\n",
    "print(f\"Model exported to {onnx_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
