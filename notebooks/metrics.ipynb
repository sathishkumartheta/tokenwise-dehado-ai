{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b88297",
   "metadata": {},
   "source": [
    "# Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89507534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Error Rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162adf3a",
   "metadata": {},
   "source": [
    "## 1. Word Error Rate (WER)\n",
    "![WER](../assets/WER.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "575f67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer,compute_measures\n",
    "\n",
    "def compute_wer(reference: str, prediction: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate (WER) between prediction and reference text.\n",
    "\n",
    "    Args:\n",
    "        reference (str): Ground truth/reference sentence.\n",
    "        prediction (str): Predicted sentence.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        float: Word Error Rate (0 to 1).\n",
    "    \"\"\"\n",
    "\n",
    "    #measures=compute_measures()\n",
    "    return wer(reference, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8daffb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "wer_rate=compute_wer(\"This is a test example\",\"This is test sample\")\n",
    "print(wer_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdcff9",
   "metadata": {},
   "source": [
    "## 2. Character Error Rate (WER)\n",
    "![WER](../assets/CER.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4888b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import cer\n",
    "\n",
    "def compute_cer(reference: str, prediction: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate (CER) between prediction and reference text.\n",
    "\n",
    "    Args:\n",
    "        prediction (str): Predicted sentence.\n",
    "        reference (str): Ground truth/reference sentence.\n",
    "\n",
    "    Returns:\n",
    "        float: Character Error Rate (0 to 1).\n",
    "    \"\"\"\n",
    "    return cer(reference, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b8299a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "cer_rate=compute_cer(\"hello world\",\"hello wurld\")\n",
    "print(cer_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbce70",
   "metadata": {},
   "source": [
    "## 3. Text Field Accuracy (WER)\n",
    "![WER](../assets/TFA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94b10e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_field_accuracy(correct_fields: int, total_fields: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes field-level accuracy percentage.\n",
    "\n",
    "    Args:\n",
    "        correct_fields (int): Number of correctly predicted fields.\n",
    "        total_fields (int): Total number of fields.\n",
    "\n",
    "    Returns:\n",
    "        float: Field accuracy (0 to 100).\n",
    "    \"\"\"\n",
    "    if total_fields == 0:\n",
    "        return 0.0\n",
    "    return (correct_fields / total_fields) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c673f",
   "metadata": {},
   "source": [
    "## 4. Document Level Accuracy (DLA)\n",
    "![WER](../assets/DLA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8781b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_Level_accuracy(correct_documents: int, total_documents: int) -> float:\n",
    "    \"\"\"\n",
    "    Computes document-level accuracy percentage.\n",
    "\n",
    "    Args:\n",
    "        correct_documents (int): Number of documents with all fields correct.\n",
    "        total_documents (int): Total number of documents.\n",
    "\n",
    "    Returns:\n",
    "        float: Document accuracy (0 to 100).\n",
    "    \"\"\"\n",
    "    if total_documents == 0:\n",
    "        return 0.0\n",
    "    return (correct_documents / total_documents) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421db8d5",
   "metadata": {},
   "source": [
    "## 5. Final Score(FA)\n",
    "![WER](../assets/FS.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15f6c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_score(wer: float, cer: float, field_acc: float, doc_acc: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the final weighted score.\n",
    "\n",
    "    Args:\n",
    "        wer (float): Word Error Rate (0 to 1).\n",
    "        cer (float): Character Error Rate (0 to 1).\n",
    "        field_acc (float): Field accuracy (0 to 100).\n",
    "        doc_acc (float): Document accuracy (0 to 100).\n",
    "\n",
    "    Returns:\n",
    "        float: Final score (0 to 100).\n",
    "    \"\"\"\n",
    "    return (\n",
    "        0.35 * (100 - wer * 100) +\n",
    "        0.35 * (100 - cer * 100) +\n",
    "        0.15 * field_acc +\n",
    "        0.15 * doc_acc\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255ac60",
   "metadata": {},
   "source": [
    "## 6. Compute Efficiency (CE)\n",
    "![WER](../assets/CE.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9ff9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_efficiency(t_avg: float, m_avg: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes the efficiency score based on average time and memory usage.\n",
    "\n",
    "    Args:\n",
    "        t_avg (float): Average processing time per document (in seconds).\n",
    "        m_avg (float): Average memory usage (in MB or GB).\n",
    "\n",
    "    Returns:\n",
    "        float: Efficiency score.\n",
    "    \"\"\"\n",
    "    if t_avg == 0 or m_avg == 0:\n",
    "        return float('inf')  # Perfect efficiency (not realistic)\n",
    "    return 1 / (t_avg * m_avg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wdehado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
