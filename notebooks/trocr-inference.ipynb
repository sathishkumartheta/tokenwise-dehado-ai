{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12079007,"sourceType":"datasetVersion","datasetId":7571578},{"sourceId":434013,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":353905,"modelId":375212}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets accelerate torchvision\n!pip install --upgrade accelerate\n!pip install --upgrade transformers\n!pip install -q sentencepiece\n!pip install -q jiwer\n!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:21:41.257267Z","iopub.execute_input":"2025-06-13T07:21:41.257813Z","iopub.status.idle":"2025-06-13T07:23:37.225166Z","shell.execute_reply.started":"2025-06-13T07:21:41.257782Z","shell.execute_reply":"2025-06-13T07:23:37.224479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\nfrom datasets import Dataset\nimport pandas as pd\nimport torch\nfrom transformers import TrOCRProcessor, default_data_collator, VisionEncoderDecoderModel\nimport evaluate\nimport numpy as np\nimport pandas as pd\nimport glob as glob\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n\n \n \nfrom PIL import Image\nfrom zipfile import ZipFile\nfrom tqdm.notebook import tqdm\nfrom dataclasses import dataclass\ndir_path = '/kaggle/input/groundtruthtext'\ntest_path = dir_path + \"/\" + 'test/gt_text'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:24:05.383404Z","iopub.execute_input":"2025-06-13T07:24:05.383730Z","iopub.status.idle":"2025-06-13T07:24:28.573800Z","shell.execute_reply.started":"2025-06-13T07:24:05.383702Z","shell.execute_reply":"2025-06-13T07:24:28.573176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\", use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:26:50.523952Z","iopub.execute_input":"2025-06-13T07:26:50.524192Z","iopub.status.idle":"2025-06-13T07:26:53.580053Z","shell.execute_reply.started":"2025-06-13T07:26:50.524175Z","shell.execute_reply":"2025-06-13T07:26:53.579493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\n# model_path = '/kaggle/input/trocr_base_400/pytorch/default/1/checkpoint-400'\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\n# processor = TrOCRProcessor.from_pretrained(\nmodel.to(\"cuda\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:27:36.972905Z","iopub.execute_input":"2025-06-13T07:27:36.973368Z","iopub.status.idle":"2025-06-13T07:27:51.535903Z","shell.execute_reply.started":"2025-06-13T07:27:36.973344Z","shell.execute_reply":"2025-06-13T07:27:51.535196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# folder_path = test_path + \"/\" + \"MIT_10\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndf = pd.DataFrame(columns=[\"folder\", \"file\", \"text\"])\nsave_cnt = 0\nfilecnt = 1\nfoldercnt = 1\nfor folder in os.listdir(test_path):\n    folder_path = test_path + \"/\" + folder\n    print(foldercnt, \". Running folder: \", folder)\n    save_cnt += 1\n    for file in os.listdir(folder_path):\n        filename = file.split(\".\")[0]\n        image_path = folder_path + \"/\" + file\n        # print(\"Running File \", filename)\n        try: \n            image = Image.open(image_path).convert(\"RGB\")\n            pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)#.half()\n            \n            # Generate output\n            generated_ids = model.generate(pixel_values)\n            \n            # Decode generated token ids to string\n            generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n            df.loc[len(df)] = [folder, filename, generated_text]\n        except:\n            print(folder, \"/\" , filename, \" is skipped\")\n            df.loc[len(df)] = [folder, filename, \"#*#\"]\n        # print(\"Predicted Text:\", generated_text)\n    \n    if save_cnt > 100:\n        df.to_csv(f\"df_{filecnt}.csv\")\n        filecnt += 1\n        save_cnt = 0\ndf.to_csv(f\"df_final.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:30:19.508306Z","iopub.execute_input":"2025-06-13T07:30:19.508978Z","iopub.status.idle":"2025-06-13T07:30:25.865435Z","shell.execute_reply.started":"2025-06-13T07:30:19.508954Z","shell.execute_reply":"2025-06-13T07:30:25.864929Z"}},"outputs":[],"execution_count":null}]}